{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxrkZWMLZB8z"
      },
      "source": [
        "# Tarea 2: Naive Bayes, Linear Models y Neural Networks\n",
        "**Procesamiento de Lenguaje Natural (CC6205-1 - Otoño 2024)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b97b4IJjZGxM"
      },
      "source": [
        "## Tarjeta de identificación\n",
        "\n",
        "**Nombres:** ```Martín Bravo, Felipe Fierro```\n",
        "\n",
        "**Fecha límite de entrega 📆:** 30/04.\n",
        "\n",
        "**Tiempo estimado de dedicación:** 4 horas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKcZMFlmZ3b9"
      },
      "source": [
        "## Instrucciones\n",
        "Bienvenid@s a la segunda tarea en el curso de Natural Language Processing (NLP). Esta tarea tiene como objetivo evaluar los contenidos teóricos de las últimas semanas de clases posteriores a la tarea 1, enfocado principalmente en **Naive Bayes**, **Linear Models** y **Neural Networks**. Si aún no has visto las clases, se recomienda visitar los links de las referencias.\n",
        "\n",
        "La tarea consta de una una parte práctica con el fín de introducirlos a la programación en Python enfocada en NLP.\n",
        "\n",
        "* La tarea es en **grupo** (maximo hasta 3 personas).\n",
        "* La entrega es a través de u-cursos a más tardar el día estipulado arriba. No se aceptan atrasos.\n",
        "* El formato de entrega es este mismo Jupyter Notebook.\n",
        "* Al momento de la revisión su código será ejecutado. Por favor verifiquen que su entrega no tenga errores de compilación.\n",
        "* Completar la tarjeta de identificación. Sin ella no podrá tener nota.\n",
        "\n",
        "> **Importante:** Esta tarea tiene varios resultados experimentales que pueden variar de acuerdo a sus propias implementaciones. No se busca que los resultados sean exactamente los mismos (por ejemplo, que el accuracy fue el mismo que el que esta en la tarea). Lo importante es que implementen sus funciones, las sepan explicar y que puedan hacer varios experimentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnTrhOKraAw2"
      },
      "source": [
        "## Material de referencia\n",
        "\n",
        "Diapositivas del curso 📄\n",
        "    \n",
        "- [Naive Bayes](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-NB.pdf)\n",
        "- [Linear Models](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-linear.pdf)\n",
        "- [Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-neural.pdf)\n",
        "\n",
        "Videos del curso 📺\n",
        "\n",
        "- Naive Bayes: [Parte 1](https://www.youtube.com/watch?v=kG9BK9Oy1hU), [Parte 2](https://www.youtube.com/watch?v=Iqte5kKHvzE), [Parte 3](https://www.youtube.com/watch?v=TSJg0_X3Abk)\n",
        "\n",
        "- Linear Models: [Parte 1](https://www.youtube.com/watch?v=zhBxDsNLZEA), [Parte 2](https://www.youtube.com/watch?v=Fooua_uaWSE), [Parte 3](https://www.youtube.com/watch?v=DqbzhdQa1eQ), [Parte 4](https://www.youtube.com/watch?v=1nfWWXqfAzA)\n",
        "\n",
        "- Neural Networks: [Parte 1](https://www.youtube.com/watch?v=oHZHA8h2xN0), [Parte 2](https://www.youtube.com/watch?v=2lXank0W6G4), [Parte 3](https://www.youtube.com/watch?v=BUDIi9qItzY), [Parte 4](https://www.youtube.com/watch?v=KKN2Ipy-vGk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmC5SqOZfv1m"
      },
      "source": [
        "## P0. Cargar un dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldJFqRlCYSa-"
      },
      "source": [
        "Importamos algunas librerias que seran utiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QytPBlbvhsU6",
        "outputId": "670780c5-beba-4f22-8507-bf8796c40f28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/martinbravodiaz/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import namedtuple\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN-mzGtAYVu-"
      },
      "source": [
        "Inicializamos el dataset con particiones de entrenamiento y test. Es un dataset de clasificacion multi-clase de oraciones. Cada oracion puede tener una unica etiqueta ?, + o -. Donde ? indica que la oracion es una pregunta, - que la oracion es negativa y + positiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "bcaT6DquoXOo"
      },
      "outputs": [],
      "source": [
        "document = namedtuple(\n",
        "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
        ")\n",
        "\n",
        "raw_train_set = [\n",
        "              ['Do you have plenty of time?', '?'],\n",
        "              ['Does she have enough money?','?'],\n",
        "              ['Did they have any useful advice?','?'],\n",
        "              ['What day is today?','?'],\n",
        "              [\"I don't have much time\",'-'],\n",
        "              [\"She doesn't have any money\",'-'],\n",
        "              [\"They didn't have any advice to offer\",'-'],\n",
        "              ['Have you plenty of time?','?'],\n",
        "              ['Has she enough money?','?'],\n",
        "              ['Had they any useful advice?','?'],\n",
        "              [\"I haven't much time\",'-'],\n",
        "              [\"She hasn't any money\",'-'],\n",
        "              [\"He hadn't any advice to offer\",'-'],\n",
        "              ['How are you?','?'],\n",
        "              ['How do you make questions in English?','?'],\n",
        "              ['How long have you lived here?','?'],\n",
        "              ['How often do you go to the cinema?','?'],\n",
        "              ['How much is this dress?','?'],\n",
        "              ['How old are you?','?'],\n",
        "              ['How many people came to the meeting?','?'],\n",
        "              ['I’m from France','+'],\n",
        "              ['I come from the UK','+'],\n",
        "              ['My phone number is 61709832145','+'],\n",
        "              ['I work as a tour guide for a local tour company','+'],\n",
        "              ['I’m not dating anyone','-'],\n",
        "              ['I live with my wife and children','+'],\n",
        "              ['I often do morning exercises at 6am','+'],\n",
        "              ['I run everyday','+'],\n",
        "              ['She walks very slowly','+'],\n",
        "              ['They eat a lot of meat daily','+'],\n",
        "              ['We were in France that day', '+'],\n",
        "              ['He speaks very fast', '+'],\n",
        "              ['They told us they came back early', '+'],\n",
        "              [\"I told her I'll be there\", '+']\n",
        "]\n",
        "tokenized_train_set = [document(words=tuple(word_tokenize(d[0].lower())), class_=d[1]) for d in raw_train_set]\n",
        "train_set = pd.DataFrame(data=tokenized_train_set)\n",
        "\n",
        "raw_test_set = [\n",
        "             ['Do you know who lives here?','?'],\n",
        "             ['What time is it?','?'],\n",
        "             ['Can you tell me where she comes from?','?'],\n",
        "             ['How are you?','?'],\n",
        "             ['I fill good today', '+'],\n",
        "             ['There is a lot of history here','+'],\n",
        "             ['I love programming','+'],\n",
        "             ['He told us not to make so much noise','+'],\n",
        "             ['We were asked not to park in front of the house','+'],\n",
        "             [\"I don't have much time\",'-'],\n",
        "             [\"She doesn't have any money\",'-'],\n",
        "             [\"They didn't have any advice to offer\",'-'],\n",
        "             ['I am not really sure','+']\n",
        "]\n",
        "tokenized_test_set = [document(words=tuple(word_tokenize(d[0].lower())), class_=d[1]) for d in raw_test_set]\n",
        "test_set = pd.DataFrame(data=tokenized_test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cIkiUTOY8Ep"
      },
      "source": [
        "Separar en X e y, donde X son oraciones tokenizadas e y es la clase a predecir (o target)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "ALoL0Xk9pJhU",
        "outputId": "07fdbc61-3322-40f4-c478-db43c918850e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>class_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>(they, told, us, they, came, back, early)</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(she, does, n't, have, any, money)</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>(i, come, from, the, uk)</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(had, they, any, useful, advice, ?)</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(does, she, have, enough, money, ?)</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>(how, old, are, you, ?)</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(what, day, is, today, ?)</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>(he, speaks, very, fast)</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>(how, are, you, ?)</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>(she, has, n't, any, money)</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        words class_\n",
              "32  (they, told, us, they, came, back, early)      +\n",
              "5          (she, does, n't, have, any, money)      -\n",
              "21                   (i, come, from, the, uk)      +\n",
              "9         (had, they, any, useful, advice, ?)      ?\n",
              "1         (does, she, have, enough, money, ?)      ?\n",
              "18                    (how, old, are, you, ?)      ?\n",
              "3                   (what, day, is, today, ?)      ?\n",
              "31                   (he, speaks, very, fast)      +\n",
              "13                         (how, are, you, ?)      ?\n",
              "11                (she, has, n't, any, money)      -"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, y_train = train_set.drop(columns=\"class_\"), train_set[\"class_\"]\n",
        "pd.concat([X_train, y_train], axis=1).sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSHQ_s9rZE6k"
      },
      "source": [
        "Cantidad de oraciones por clase:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "djD7TovViVuB",
        "outputId": "7c634105-1210-47b0-a5a0-e00ba36731a6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class_</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>+</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>?</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        words\n",
              "class_       \n",
              "+          13\n",
              "-           7\n",
              "?          14"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set.groupby(\"class_\").count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQNS_HatZKBg"
      },
      "source": [
        "(X, y) para el conjunto de test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "5i0O-7-IpLbX",
        "outputId": "0dde8012-aba7-40d3-83c1-76b237609a05"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>class_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(there, is, a, lot, of, history, here)</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(i, do, n't, have, much, time)</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(i, love, programming)</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(i, fill, good, today)</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(can, you, tell, me, where, she, comes, from, ?)</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>(she, does, n't, have, any, money)</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(how, are, you, ?)</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(what, time, is, it, ?)</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(do, you, know, who, lives, here, ?)</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(we, were, asked, not, to, park, in, front, of...</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                words class_\n",
              "5              (there, is, a, lot, of, history, here)      +\n",
              "9                      (i, do, n't, have, much, time)      -\n",
              "6                              (i, love, programming)      +\n",
              "4                              (i, fill, good, today)      +\n",
              "2    (can, you, tell, me, where, she, comes, from, ?)      ?\n",
              "10                 (she, does, n't, have, any, money)      -\n",
              "3                                  (how, are, you, ?)      ?\n",
              "1                             (what, time, is, it, ?)      ?\n",
              "0                (do, you, know, who, lives, here, ?)      ?\n",
              "8   (we, were, asked, not, to, park, in, front, of...      +"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test, y_test = test_set.drop(columns=\"class_\"), test_set[\"class_\"]\n",
        "pd.concat([X_test, y_test], axis=1).sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdwzchFGZNmf"
      },
      "source": [
        "Cantidad de oraciones por clase en el conjunto de test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "5FXC-oNNpQGw",
        "outputId": "dc46f6c2-eb35-4baf-971f-26f64ca865be"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class_</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>+</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>?</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        words\n",
              "class_       \n",
              "+           6\n",
              "-           3\n",
              "?           4"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set.groupby(\"class_\").count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTfP4UGXZUSX"
      },
      "source": [
        "**Importante:** Hasta el momento hemos creado nuestros conjuntos de train y test. A continuacion ustedes deben implementar tres modelos de clasificacion: Naive-bayes, Linear Model y Neural Network. Aqui va un resumen de cada pregunta y lo que se les pide implementar:\n",
        "\n",
        "* P1: Naive-bayes\n",
        " - Implementar `fit` y `predict`\n",
        " - Entrenar\n",
        " - Evaluar\n",
        "\n",
        "* P2: Linear Model\n",
        " - Implementar `fit` con *on-line gradient descent* y `predict`\n",
        " - Entrenar\n",
        " - Evaluar\n",
        "\n",
        "* P3: Neural Network\n",
        " - Implementar un iterador de datos con `datasets` y `dataloaders`\n",
        " - Implementar una red neuronal con `pytorch`\n",
        " - Implementar loop de entrenamiento de una NN\n",
        " - Entrenar\n",
        " - Evaluar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxYRIr-Ob010"
      },
      "source": [
        "## P1. Implementar y evaluar Multinomial Naive-Bayes (2 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWZGccxjcFE-"
      },
      "source": [
        "### Clase para clasificador\n",
        "\n",
        "Cree una clase MyMultinomialNB que en su inicializador reciba el parámetro alpha para su clasficador.\n",
        "\n",
        "Además, debe implementar los métodos `fit(X, y)`y `predict(X)`.\n",
        "\n",
        "```python\n",
        "class MyMultinomialNB():\n",
        "  def __init__(self, alpha, ...):\n",
        "    ...\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    ...\n",
        "  \n",
        "  def predict(self, X):\n",
        "    ...\n",
        "    return prediction\n",
        "```\n",
        "Para computar el entrenamiento de nuestro clasificador debemos:\n",
        "- extraer el vocabulario,\n",
        "- determinar las probabilidades $p(c_j)$ para cada una de las clases posibles,\n",
        "- determinar las probabilidades $p(w_i|c_j)$ para cada una de las palabras y cada una de las clases.\n",
        "\n",
        "Para lograr lo anterior, también deberán implementar el método `predict_proba(X)`:\n",
        "\n",
        "```python\n",
        "  def predict_proba(self, X):\n",
        "    return prob\n",
        "```\n",
        "\n",
        "**Underflow prevention:** En vez de hacer muchas multiplicaciones de `float`s, reemplácenlas por sumas de logaritmos para prevenir errores de precisión. (Revisen la diapo 26 de las slides).\n",
        "\n",
        "En su implementación deben considerar la tecnica de *Laplace Smoothing* vista en clases. Especificamente considere que su clase `MyMultinomialNB` reciba un parámetro `alpha` no negativo (es decir, mayor o igual a cero). De tal forma que el la probabilidad de una palabra $w$ dado la clase $c$ viene dado por lo siguiente:\n",
        "\n",
        "$$\n",
        "p_\\alpha (w|c) = \\frac{\\#(w, c) + \\alpha}{N + \\alpha |V|}\n",
        "$$\n",
        "\n",
        "donde $\\alpha$ es el parámetro `alpha` de *Laplace Smoothing*. Mientras que los otras notaciones corresponden a\n",
        "\n",
        "* $\\#(w, c)$ numero de veces que ocurre la palabra $w$ en documentos con la clase $c$ (pensar en un gran documento $D_c$ que concatena todos los documentos de clase $c$ y luego calcula la frecuencia de la palabra $w$ en $D_c$),\n",
        "* $N$ es igual a $\\sum \\{\\#(w', c): w' \\in V\\}$ donde $V$ es el vocabulario,\n",
        "* $|V|$ tamaño del vocabulario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0hFPME1aU9o"
      },
      "source": [
        "### Implementación (1.5 pts.)\n",
        "\n",
        "Escriba aquí la implementación de la clase `MyMultinomialNB`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JsvJLWy6iyg",
        "outputId": "714971e4-157e-44ba-ec77-09a5e3dfd901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('hola', 'como', 'estás')\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([1,2,2,3,3,1,1])\n",
        "\n",
        "print(tuple(word_tokenize(\"hola como estás\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "JU5i9li8g7CS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MyMultinomialNB():\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha  # Parámetro de suavizado Laplaciano\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Conteo de clases y vocabulario\n",
        "        self.classes, class_counts = np.unique(y, return_counts=True)\n",
        "        self.class_counts = dict(zip(self.classes, class_counts))\n",
        "        self.vocab = set()\n",
        "        self.class_word_counts = {c: {} for c in self.classes}\n",
        "\n",
        "        # Conteo de palabras por clase\n",
        "        for c in self.classes:\n",
        "            mask = (y == c)\n",
        "            documents = X[mask] # Documentos de la clase c\n",
        "            word_counts = {}\n",
        "            for document in documents.values.flatten():\n",
        "                for word in document:\n",
        "                    self.vocab.add(word)\n",
        "                    word_counts[word] = word_counts.get(word, 0) + 1\n",
        "            self.class_word_counts[c] = word_counts\n",
        "\n",
        "        # Calcula la probabilidad de cada clase\n",
        "        total_documents = len(y)\n",
        "        self.class_probs = {c: count / total_documents for c, count in self.class_counts.items()}\n",
        "\n",
        "        # Calcula la probabilidad condicional de cada palabra dado cada clase\n",
        "        self.word_probs = {}\n",
        "        for c in self.classes:\n",
        "            total_words_in_class = sum(self.class_word_counts[c].values())\n",
        "            self.word_probs[c] = {}\n",
        "            for word in self.vocab:\n",
        "                word_count = self.class_word_counts[c].get(word, 0)\n",
        "                self.word_probs[c][word] = (word_count + self.alpha) / (total_words_in_class + self.alpha * len(self.vocab))\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = []\n",
        "        for document in X.values.flatten():\n",
        "            max_prob = -np.inf\n",
        "            predicted_class = None\n",
        "            for c in self.classes:\n",
        "                log_prob = np.log(self.class_probs[c])\n",
        "                for word in document:\n",
        "                    if word in self.vocab:\n",
        "                        log_prob += np.log(self.word_probs[c].get(word, 1e-10))  # Evitar log(0)\n",
        "                if log_prob > max_prob:\n",
        "                    max_prob = log_prob\n",
        "                    predicted_class = c\n",
        "            pred.append(predicted_class)\n",
        "        return pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY0j4Ruy0Cxr"
      },
      "source": [
        "### Entrenamiento (0.2 pts.)\n",
        "A continuación, inicialicen y entrenen (ajusten) su clasificador con los datos de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "EuLl606ZjN4U"
      },
      "outputs": [],
      "source": [
        "nb_model = MyMultinomialNB(alpha=0.1)\n",
        "nb_model.fit(X_train, y_train);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvTzP9vPjfFH"
      },
      "source": [
        "Pruébenlo utilizando el método `predict()` que implementaron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "j5wph2IQuAzo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQtmK4jfi8mP",
        "outputId": "ae8038bb-8d41-400a-84a4-80c304909119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['?', '?', '?', '?', '-', '-', '-', '?', '?', '?', '-', '-', '-', '?', '?', '?', '?', '?', '?', '?', '+', '+', '+', '+', '-', '+', '+', '+', '+', '+', '+', '+', '+', '+']\n"
          ]
        }
      ],
      "source": [
        "# Predict train-set\n",
        "y_pred = nb_model.predict(X_train)\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "702uKpJLtPJj",
        "outputId": "95937747-9f12-4e37-f9f8-f42011874ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           +       1.00      1.00      1.00        13\n",
            "           -       1.00      1.00      1.00         7\n",
            "           ?       1.00      1.00      1.00        14\n",
            "\n",
            "    accuracy                           1.00        34\n",
            "   macro avg       1.00      1.00      1.00        34\n",
            "weighted avg       1.00      1.00      1.00        34\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Metricas en el conjunto de train\n",
        "print(classification_report(y_train, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzRqkuso1PWT"
      },
      "source": [
        "### Evaluación (0.3 pts.)\n",
        "\n",
        "Ahora probarán el funcionamiento de su clasificador con un conjunto de test.  Habiendo entrenado su clasificador, clasifiquen los documentos del conjunto de prueba `test_set` usando el método `predict`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjMuND9djskK",
        "outputId": "59abe4a1-8d13-4639-de6a-5be697e182ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           +       1.00      0.67      0.80         6\n",
            "           -       0.60      1.00      0.75         3\n",
            "           ?       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           0.85        13\n",
            "   macro avg       0.87      0.89      0.85        13\n",
            "weighted avg       0.91      0.85      0.85        13\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = nb_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjvzyLHJVFdg"
      },
      "source": [
        "Comenten sus resultados. Estudien que ocurre para alpha=0, 1 y L donde L es un numero muy grande.\n",
        "\n",
        "```\n",
        "Comentar aquí.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNSdg__vb6J7"
      },
      "source": [
        "## P2. Implementar y evaluar Linear Models (2 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSlhzDQ5ac3u"
      },
      "source": [
        "### Clase para clasificador\n",
        "\n",
        "Cree una clase MyLinearModel para su clasficador. Debe implementar los métodos `fit(X, y, learning_rate, epochs)`y `predict(X)`.\n",
        "\n",
        "```python\n",
        "class MyLinearModel():\n",
        "  def __init__(self, ...):\n",
        "    ...\n",
        "\n",
        "  def fit(self, X, y, learning_rate, epochs):\n",
        "    ...\n",
        "  \n",
        "  def predict(self, X):\n",
        "    ...\n",
        "    return prediction\n",
        "```\n",
        "\n",
        "El modelo lineal que debe implementar viene dado por:\n",
        "$$\n",
        "\\vec{\\hat{y}} = \\text{softmax}(\\vec{x} \\cdot W + \\vec{b})\\\\\n",
        "\\vec{\\hat{y}}_{[i]} = \\frac{\\exp{z_i}}{\\sum_{j} \\exp{z_j}}\\\\\n",
        "z_i = \\vec{x} \\cdot W_{[:, i]} + \\vec{b}_{[i]}\n",
        "$$\n",
        "donde $\\vec{x}$ es un documento representado con bolsas de palabras (BoW), $W$ es la matriz de pesos y $\\vec{b}$ el bias.\n",
        "\n",
        "El modelo linea debe ajustarlo considerando como objetivo minimizar la cross-entropy loss, es decir:\n",
        "\n",
        "$$\n",
        "L_\\text{cross-entropy}(\\vec{\\hat{y}}, \\vec{y}) = - \\sum_i \\vec{y}_{[i]} \\log{ \\left( \\vec{\\hat{y}}_{[i]} \\right) }\n",
        "$$\n",
        "\n",
        "Para representar un documento `(i, am, not, really, sure)` vectorialmente, utilice `CountVectorizer` de sklearn. De esta manera, el documento queda representado como sigue:\n",
        "\n",
        "|    |   i |   he |   am |   are |   not |   yes |   really |   sure |\n",
        "|---:|----:|-----:|-----:|------:|------:|------:|---------:|-------:|\n",
        "|  0 |   1 |    0 |    1 |     0 |     1 |     0 |        1 |      1 |\n",
        "\n",
        "**Observación:** Si el documento repite palabras entonces tendrá un número mayor a 1. Si el documento no tiene la palabra entonces tiene un 0. Pensar que las palabras `(he, are, yes)` provienen de otros documentos. Recuerde que el `CountVectorizer` se entrena con más de un documento (es decir, un corpus). Aquí debe usar el conjunto de train.\n",
        "\n",
        "El método `fit(X, y, learning_rate, epochs)` debe ajustar un `CountVectorizer` para representar vectorialmente el documento. Debe guardar el `CountVectorizer` para cuando quiera hacer predicciones. Dentro del método `fit(X, y, learning_rate, epochs)` debe implementar *On-line gradient descent* (visto en clases), es decir, descenso de gradiente usando un data-point por iteración. Su método debe ser capaz de recibir un `learning_rate` para ponderar el gradiente en cada iteración y fijar un número de `epochs`. Luego de entrenar debe guardar los pesos de su modelo lineal, es decir, $(W, \\vec{b})$.\n",
        "\n",
        "En el algoritmo de descenso de gradiente usando un data-point por iteración, o *On-line gradient descent*, debe implementar manualmente las derivadas. Como conoce el modelo lineal y la funcion objetivo, entonces puede calcular manualmente las derivadas. Para ejemplificar, en cada paso del algoritmo de optimizacion debe actualizar los pesos $(W, \\vec{b})$ del siguiente modo:\n",
        "\n",
        "$$\n",
        "W \\leftarrow W - \\lambda \\nabla_{W} L_\\text{cross-entropy}\\\\\n",
        "\\vec{b} \\leftarrow \\vec{b} - \\lambda \\nabla_{\\vec{b}} L_\\text{cross-entropy}\\\\\n",
        "$$\n",
        "donde $\\lambda$ es el parámetro `learning_rate`, $\\nabla_{W} L_\\text{cross-entropy}$ el gradiente de la Loss con repecto a la matriz de pesos $W$ y $\\nabla_{\\vec{b}} L_\\text{cross-entropy}$ para el bias $\\vec{b}$.\n",
        "\n",
        "Para implementar el algoritmo *On-line gradient descent* les recomendamos (no es obligatorio hacerlo de este modo) definir una función `get_derivative_W(x, y_target, y_pred, n_classes)` que calcule $\\nabla_{W} L_\\text{cross-entropy}$ y lo mismo con una función `get_derivative_b(y_target, y_pred, n_classes)` que calcule $\\nabla_{\\vec{b}} L_\\text{cross-entropy}$.\n",
        "\n",
        "Para implementar el método `predict(self, X)` debera usar su `CountVectorizer` definido en `fit(X, y, learning_rate, epochs)` para representar del mismo modo cualquier documento tanto en train como en test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqJhGTQS0rpv"
      },
      "source": [
        "### Implementación (1.5 pts.)\n",
        "Implemente un modelo lineal con métodos `fit(X, y)` y `predict(X)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "L_\\text{cross-entropy}(\\vec{\\hat{y}}, \\vec{y}) = - \\sum_i \\vec{y}_{[i]} \\log{ \\left( \\vec{\\hat{y}}_{[i]} \\right) }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "= - \\sum_i \\frac{\\exp{z_i}}{\\sum_{j} \\exp{z_j}} \\log{ \\left( \\frac{\\exp{z_i}}{\\sum_{j} \\exp{z_j}} \\right) }\n",
        "$$\n",
        "\n",
        "$$\n",
        "= - \\sum_i \\frac{\\exp{\\vec{x} \\cdot W_{[:, i]} + \\vec{b}_{[i]}}}{\\sum_{j} \\exp{\\vec{x} \\cdot W_{[:, j]} + \\vec{b}_{[j]}}} \\log{ \\left( \\frac{\\exp{\\vec{x} \\cdot W_{[:, i]} + \\vec{b}_{[i]}}}{\\sum_{j} \\exp{\\vec{x} \\cdot W_{[:, j]} + \\vec{b}_{[j]}}} \\right) }\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\nabla_W L = - \\sum_i \\frac{\\exp{z_i}}{\\sum_{j} \\exp{z_j}} \\nabla_W \\log{ \\left( \\frac{\\exp{z_i}}{\\sum_{j} \\exp{z_j}} \\right) }\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "lO5zE2ArjifE"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "class MyLinearModel():\n",
        "    def __init__(self):\n",
        "        self.vectorizer = CountVectorizer()\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y, learning_rate, epochs, verbose=False):\n",
        "\n",
        "        # Pasar los documentos a frases para poder vectorizarlos\n",
        "        X = X[\"words\"].apply(lambda x: \" \".join(x)).values\n",
        "\n",
        "        # Pasar los documentos a una representación vectorial\n",
        "        X = self.vectorizer.fit_transform(X).toarray().T\n",
        "\n",
        "        # Pasar los valores de predicción a números\n",
        "        y = y.apply(lambda x: 0 if x == '-' else 1 if x == '+' else 2).values\n",
        "\n",
        "        # Inicializar pesos\n",
        "        self.W = np.random.randn(3, X.shape[0])\n",
        "        self.b = np.random.randn(3)\n",
        "\n",
        "        # Repetimos el proceso epoch cantidad de veces\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # Para cada documento\n",
        "            for x_i, it in zip(X.T, y):\n",
        "\n",
        "                # One-hot encoding\n",
        "                y_i = np.zeros(3)\n",
        "                y_i[it] = 1\n",
        "\n",
        "                # Calculamos la predicción\n",
        "                y_i_pred = softmax(self.W @ x_i + self.b)\n",
        "\n",
        "                # Calculamos la función de pérdida cross-entropy\n",
        "                loss = -np.sum(y_i * np.log(y_i_pred))\n",
        "\n",
        "                # Actualizamos los pesos\n",
        "                self.W -= learning_rate * np.outer(y_i_pred - y_i, x_i)\n",
        "                self.b -= learning_rate * (y_i_pred - y_i)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "            \n",
        "        return \n",
        "    \n",
        "    def predict(self, X):\n",
        "        # Pasar los documentos a frases para poder vectorizarlos\n",
        "        X = X[\"words\"].apply(lambda x: \" \".join(x)).values\n",
        "\n",
        "        # Pasar los documentos a una representación vectorial\n",
        "        X = self.vectorizer.transform(X).toarray().T\n",
        "\n",
        "        preds = []\n",
        "        # Realizar la predicción\n",
        "        for x_i in X.T:\n",
        "\n",
        "            # Calculamos la predicción\n",
        "            y_i_pred = softmax(self.W @ x_i + self.b)\n",
        "\n",
        "            # Obtenemos la mayor probabilidad\n",
        "            it = np.argmax(y_i_pred)\n",
        "        \n",
        "            # Pasar los valores de predicción a letras\n",
        "            pred = '+' if it == 1 else '-' if it == 0 else '?'\n",
        "            preds.append(pred)\n",
        "            \n",
        "        return preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANT3rY1QkhyZ"
      },
      "source": [
        "### Entrenamiento (0.2 pts.)\n",
        "Inicialicen y entrenen su clasificador con los datos de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXY8ACuVIRb9",
        "outputId": "e5776a5f-27e9-4d78-cd16-ec3d35fa99db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.3957748723023839\n",
            "Epoch 1, Loss: 0.30757547291453147\n",
            "Epoch 2, Loss: 0.24743353935685344\n",
            "Epoch 3, Loss: 0.2049797007740419\n",
            "Epoch 4, Loss: 0.1740520568874803\n",
            "Epoch 5, Loss: 0.15087688710416158\n",
            "Epoch 6, Loss: 0.1330561469598279\n",
            "Epoch 7, Loss: 0.11901966388586889\n",
            "Epoch 8, Loss: 0.10771686448787511\n",
            "Epoch 9, Loss: 0.09843289068895368\n",
            "Epoch 10, Loss: 0.09067386014641597\n",
            "Epoch 11, Loss: 0.08409336889317488\n",
            "Epoch 12, Loss: 0.07844469029498186\n",
            "Epoch 13, Loss: 0.07354930306568618\n",
            "Epoch 14, Loss: 0.06927586118445113\n"
          ]
        }
      ],
      "source": [
        "linear_model = MyLinearModel()\n",
        "linear_model.fit(\n",
        "    X_train, y_train,\n",
        "    learning_rate=0.02,\n",
        "    epochs=15,\n",
        "    verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDUL-ubzh04B"
      },
      "source": [
        "Pruébenlo utilizando el método `predict()` que implementaron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JKjK3pUwL1w",
        "outputId": "d1fceb7a-70af-4029-a4c7-1a85f2d4c686"
      },
      "outputs": [],
      "source": [
        "# Predict train-set\n",
        "y_pred = linear_model.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i6Qiu0RwP-w",
        "outputId": "1d9476ef-8d91-4531-c7f6-2ee2b425044c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           +       0.83      0.77      0.80        13\n",
            "           -       0.75      0.86      0.80         7\n",
            "           ?       0.79      0.79      0.79        14\n",
            "\n",
            "    accuracy                           0.79        34\n",
            "   macro avg       0.79      0.80      0.80        34\n",
            "weighted avg       0.80      0.79      0.79        34\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Metricas en el conjunto de train\n",
        "print(classification_report(y_train, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGzafGqr1GBX"
      },
      "source": [
        "### Evaluación (0.3 pts.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Tj_g-Qh7hD"
      },
      "source": [
        "Ahora probarán el funcionamiento de su clasificador con un conjunto de test.  Habiendo entrenado su clasificador, clasifiquen los documentos del conjunto de prueba `test_set` usando el método `predict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-03hyMeQ94e0",
        "outputId": "ada077ba-2ba1-49c3-87c0-3a7a66e99c13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           +       1.00      0.17      0.29         6\n",
            "           -       0.50      0.67      0.57         3\n",
            "           ?       0.50      1.00      0.67         4\n",
            "\n",
            "    accuracy                           0.54        13\n",
            "   macro avg       0.67      0.61      0.51        13\n",
            "weighted avg       0.73      0.54      0.47        13\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = linear_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gVYYq5h_CU"
      },
      "source": [
        "Comenten sus resultados. Estudien que ocurre para al menos tres combinaciones de learning rates y epochs, por ejemplo `learning_rate, epochs = (0.02, 15), (0.1, 10), (0.005, 30)`.\n",
        "\n",
        "```\n",
        "Comentar aquí.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate: 0.1, Epochs: 10\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           +       0.75      0.50      0.60         6\n",
            "           -       0.60      1.00      0.75         3\n",
            "           ?       0.50      0.50      0.50         4\n",
            "\n",
            "    accuracy                           0.62        13\n",
            "   macro avg       0.62      0.67      0.62        13\n",
            "weighted avg       0.64      0.62      0.60        13\n",
            "\n",
            "\n",
            "Learning rate: 0.1, Epochs: 30\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           +       0.83      0.83      0.83         6\n",
            "           -       0.75      1.00      0.86         3\n",
            "           ?       1.00      0.75      0.86         4\n",
            "\n",
            "    accuracy                           0.85        13\n",
            "   macro avg       0.86      0.86      0.85        13\n",
            "weighted avg       0.87      0.85      0.85        13\n",
            "\n",
            "\n",
            "Learning rate: 0.01, Epochs: 15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           +       0.67      0.67      0.67         6\n",
            "           -       0.67      0.67      0.67         3\n",
            "           ?       0.25      0.25      0.25         4\n",
            "\n",
            "    accuracy                           0.54        13\n",
            "   macro avg       0.53      0.53      0.53        13\n",
            "weighted avg       0.54      0.54      0.54        13\n",
            "\n",
            "\n",
            "Learning rate: 0.001, Epochs: 15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           +       0.00      0.00      0.00       6.0\n",
            "           -       0.00      0.00      0.00       3.0\n",
            "           ?       0.00      0.00      0.00       4.0\n",
            "\n",
            "    accuracy                           0.00      13.0\n",
            "   macro avg       0.00      0.00      0.00      13.0\n",
            "weighted avg       0.00      0.00      0.00      13.0\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "combs = [(0.1, 10), (0.1, 30), (0.01, 15), (0.001, 15)]\n",
        "\n",
        "for lr, ep in combs:\n",
        "    linear_model = MyLinearModel()\n",
        "    linear_model.fit(\n",
        "        X_train, y_train,\n",
        "        learning_rate=lr,\n",
        "        epochs=ep,\n",
        "        verbose=False)\n",
        "\n",
        "    y_pred = linear_model.predict(X_test)\n",
        "    print(f\"Learning rate: {lr}, Epochs: {ep}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos notar que a medida que disminuimos el learning rate, el modelo disminuye su precisión. Por otro lado, a medida que aumentamos el número de epochs, el modelo mejora su precisión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a71d7NNCfTna"
      },
      "source": [
        "## P3. Implementar y evaluar Neural Networks (2 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6eCZbdHkVvt"
      },
      "source": [
        "### Especificaciones del clasificador\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vSXJm5I61m6w0RHTwBL-iMyeFLr2wXBrKNYxdU8Bu1ymuCFPD9dAPsCzPfvIqwSr8uCiYvWMdnGy1if/pub?w=818&h=503\" >\n",
        "\n",
        "En esta última pregunta, ustedes deberánimplementar y evaluar redes neuronales (como la de la figura de arriba). Para esto debera implementar tres secciones principales:\n",
        "\n",
        "1. Sección iterador,\n",
        "2. Sección modelo, y\n",
        "3. Sección loop de entrenamiento.\n",
        "\n",
        "> **Recomendación:** Para completar esta pregunta puede guiarse del Auxiliar 2 (clase del día 18/04)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEtleHcI94Ae"
      },
      "source": [
        "*Seccion iterador*\n",
        "\n",
        "Para ayudarnos a con el entrenamiento y testing, vamos a utilizar las clases `Dataset` y `DataLoader` de `pytorch` ([ver documentación](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)). En esta sección deberá implementar un contenedor para su conjunto de datos usando la clase `Dataset` de `pytorch`. Para esto deberá crear su propia clase `MyDataset` para gestionar los datos. Ésto le permitirá iterar sobre el conjunto mediante el iterador `DataLoader` de `pytorch` y entrenar sin hacer ningún pre-procesamiento extra a los datos.\n",
        "\n",
        "**Observación:** Si considera por funcionalidad cambiar los parámetros de la clase `MyDataset` puede hacerlo. Asimismo, puede definir otros parámetros para los métodos de su clase.\n",
        "\n",
        "\n",
        "```python\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, bow_cols):\n",
        "      ...\n",
        "\n",
        "    def __len__(self):\n",
        "      ...\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      ...\n",
        "      return x_bow, label\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "758StzD796q6"
      },
      "source": [
        "*Sección modelo*\n",
        "\n",
        "En esta sección deberán implementar la clase `MyNeuralNetwork` del modulo de `pytorch` llamado `nn.Module` con el proposito de diseñar una red neuronal como la figura de arriba. Para mas detalle sobre las redes ver Clase NLP-Neural.pdf Slide número 8.\n",
        "\n",
        "**Observación:** La figura de arriba es solo ilustrativa, ustedes pueden variar la dimension input y output de la capa oculta. Sin embargo deben mantener fija la dimension de la entrada y salida de la red. La entrada depende del tamaño del vocabulario. Mientras que la salida depende de la cantidad de clases de su problema de clasificación (en nuestro caso igual a 3).\n",
        "\n",
        "Es importante que la clase `MyNeuralNetwork` tenga implementadas apropiadamente el `__init__` con las dimensiones y el `forward` con entrada tipo BoW retornando el último estado de la red (output layer). En el `forward` recomendamos utilizar funciones de activación tipo `nn.ReLU`. Sin embargo, no es completamente obligatorio por lo que pueden usar otras.\n",
        "\n",
        "```python\n",
        "class MyNeuralNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dim_vocab,\n",
        "                 num_classes,\n",
        "                 dim_hidden_input,\n",
        "                 dim_hidden_output):\n",
        "\n",
        "        super(MyNeuralNetwork, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "      ...\n",
        "\n",
        "    def forward(self, xs_bow):\n",
        "      ...\n",
        "      return last_state\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K2ocUUF99X5"
      },
      "source": [
        "*Sección loop de entrenamiento*\n",
        "\n",
        "En esta sección deberán implementar el loop de entrenamiento de su red neuronal. Para esto, primero deben definir un `criterion`, en nuestro caso `nn.CrossEntropyLoss()` con la libreria de `pytorch`. Sucesivamente debera definir un optimizador, en nuestro caso `optim.SGD` desde el modulo `optim` de `pytorch`.\n",
        "\n",
        "El loop de entrenamiento debe seguir la siguiente estructura:\n",
        "```python\n",
        "for epoch in range(epochs):\n",
        "  for (xs_bow, labels) in train_loader:\n",
        "    ...\n",
        "```\n",
        "\n",
        "donde `train_loader` proviene del iterador generado en la \"sección iterador\".\n",
        "\n",
        "Dentro de \"doble for\" debera conjugar apropiadamente `opti.zero_grad()`, `loss = criterion(...)`, `loss.backward()` y `opti.step()` con tal de entrenar correctamente su red neuronal. Incluso entrenar, ya que a veces si no se hace de forma correcta entonces tristemente ¡su red no entrena!\n",
        "\n",
        "> **Recomendación:** Puede guiarse del Auxiliar 2 para implementar el loop de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi1fdYw3kd-j"
      },
      "source": [
        "### Preparación de la GPU y los datos de train/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-iWhEKOtgp0"
      },
      "source": [
        "Importar la libreria `pytorch` y `numpy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "zWBi-34WAKnC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8mprHO5tbJv"
      },
      "source": [
        "Verificar que esta usando GPU. Sino, dirígase a **Runtime > Change runtime type** y seleccione la opción **T4 GPU**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZU9DqaeAX-o",
        "outputId": "47d81b4c-ea07-4b53-f402-00551f5930a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3BvxVN-uefp"
      },
      "source": [
        "Preparación de los conjuntos train y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "MfXfl3SZBSmF"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>'ll</th>\n",
              "      <th>61709832145</th>\n",
              "      <th>6am</th>\n",
              "      <th>?</th>\n",
              "      <th>a</th>\n",
              "      <th>advice</th>\n",
              "      <th>and</th>\n",
              "      <th>any</th>\n",
              "      <th>anyone</th>\n",
              "      <th>are</th>\n",
              "      <th>...</th>\n",
              "      <th>we</th>\n",
              "      <th>were</th>\n",
              "      <th>what</th>\n",
              "      <th>wife</th>\n",
              "      <th>with</th>\n",
              "      <th>work</th>\n",
              "      <th>you</th>\n",
              "      <th>’</th>\n",
              "      <th>class_</th>\n",
              "      <th>int_class_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>+</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>+</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>+</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>+</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    'll  61709832145  6am    ?    a  advice  and  any  anyone  are  ...   we  \\\n",
              "32  0.0          0.0  0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  ...  0.0   \n",
              "0   0.0          0.0  0.0  1.0  0.0     0.0  0.0  0.0     0.0  0.0  ...  0.0   \n",
              "1   0.0          0.0  0.0  1.0  0.0     0.0  0.0  0.0     0.0  0.0  ...  0.0   \n",
              "21  0.0          0.0  0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  ...  0.0   \n",
              "10  0.0          0.0  0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  ...  0.0   \n",
              "7   0.0          0.0  0.0  1.0  0.0     0.0  0.0  0.0     0.0  0.0  ...  0.0   \n",
              "25  0.0          0.0  0.0  0.0  0.0     0.0  1.0  0.0     0.0  0.0  ...  0.0   \n",
              "26  0.0          0.0  1.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  ...  0.0   \n",
              "24  0.0          0.0  0.0  0.0  0.0     0.0  0.0  0.0     1.0  0.0  ...  0.0   \n",
              "8   0.0          0.0  0.0  1.0  0.0     0.0  0.0  0.0     0.0  0.0  ...  0.0   \n",
              "\n",
              "    were  what  wife  with  work  you    ’  class_  int_class_  \n",
              "32   0.0   0.0   0.0   0.0   0.0  0.0  0.0       +           1  \n",
              "0    0.0   0.0   0.0   0.0   0.0  1.0  0.0       ?           0  \n",
              "1    0.0   0.0   0.0   0.0   0.0  0.0  0.0       ?           0  \n",
              "21   0.0   0.0   0.0   0.0   0.0  0.0  0.0       +           1  \n",
              "10   0.0   0.0   0.0   0.0   0.0  0.0  0.0       -           2  \n",
              "7    0.0   0.0   0.0   0.0   0.0  1.0  0.0       ?           0  \n",
              "25   0.0   0.0   1.0   1.0   0.0  0.0  0.0       +           1  \n",
              "26   0.0   0.0   0.0   0.0   0.0  0.0  0.0       +           1  \n",
              "24   0.0   0.0   0.0   0.0   0.0  0.0  1.0       -           2  \n",
              "8    0.0   0.0   0.0   0.0   0.0  0.0  0.0       ?           0  \n",
              "\n",
              "[10 rows x 102 columns]"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow = CountVectorizer(tokenizer=lambda x: list(x), preprocessor=lambda x: x, token_pattern=None)\n",
        "\n",
        "bow_train = pd.DataFrame(\n",
        "    bow.fit_transform(train_set[\"words\"]).toarray(),\n",
        "    columns=bow.get_feature_names_out()\n",
        ")\n",
        "bow_test = pd.DataFrame(\n",
        "    bow.transform(test_set[\"words\"]).toarray(),\n",
        "    columns=bow.get_feature_names_out()\n",
        ")\n",
        "\n",
        "bow_label_train = bow_train.astype(float).copy()\n",
        "bow_label_test = bow_test.astype(float).copy()\n",
        "\n",
        "map_from_class_to_int = {\n",
        "    \"?\": 0,\n",
        "    \"+\": 1,\n",
        "    \"-\": 2\n",
        "}\n",
        "\n",
        "bow_label_train[\"class_\"] = train_set[\"class_\"]\n",
        "bow_label_train[\"int_class_\"] = train_set[\"class_\"].apply(lambda x: map_from_class_to_int[x])\n",
        "\n",
        "bow_label_test[\"class_\"] = test_set[\"class_\"]\n",
        "bow_label_test[\"int_class_\"] = test_set[\"class_\"].apply(lambda x: map_from_class_to_int[x])\n",
        "\n",
        "bow_label_train.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWdzeqbb1reG"
      },
      "source": [
        "### Implementación (1.7 pts.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rvXmHBqlUfR"
      },
      "source": [
        "#### Iterador de conjunto de datos\n",
        "Implemente su clase `MyDataset` para acceder al dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "7s8tFWCuBCin"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, bow_cols):\n",
        "        self.data = data\n",
        "        self.bow_cols = bow_cols\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = int(self.data.loc[index, \"int_class_\"])\n",
        "        x_bow = torch.tensor(self.data.loc[index, self.bow_cols]. # Obtenemos el vector x_{index}\n",
        "        values.astype(float)).to(torch.float32) # y lo convertimos a tensor de float32\n",
        "        return x_bow, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK7lLpkXup49"
      },
      "source": [
        "Inicializar cada dataloader con sus cotenedor datos para train y test, y número de batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "UeJi-t9dCi4p"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    MyDataset(data = bow_label_train, bow_cols = bow_train.columns),\n",
        "    batch_size = 5, num_workers = 1, shuffle=False)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    MyDataset(data = bow_label_test, bow_cols = bow_test.columns),\n",
        "    batch_size = 5, num_workers = 1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DdOpdUnicoc"
      },
      "source": [
        "Ejemplo de prueba para un batch de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wGaYRzIaxr4",
        "outputId": "72b59685-b1c1-42fb-f2e2-052881edbd3d"
      },
      "outputs": [],
      "source": [
        "# batch = next(iter(train_loader))\n",
        "# print( batch )\n",
        "# print( batch[0].shape, sample[1].shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2qotKTXmDue"
      },
      "source": [
        "#### Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwBRaPKLhdoU"
      },
      "source": [
        "Implemente a continuación su red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "B_g0kYDXDFsW"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class MyNeuralNetwork(nn.Module):\n",
        "  # Implementar aquí su NN\n",
        "    def __init__(self,\n",
        "                 dim_vocab,\n",
        "                 num_classes,\n",
        "                 dim_hidden_input,\n",
        "                 dim_hidden_output):\n",
        "      \"\"\"Inicializa la red neuronal\n",
        "\n",
        "      Returns:\n",
        "        None\n",
        "      \"\"\"\n",
        "      super(MyNeuralNetwork, self).__init__()\n",
        "    \n",
        "      torch.manual_seed(42)\n",
        "\n",
        "      # Definimos las capas del modelo\n",
        "\n",
        "      # Primera Capa\n",
        "      self.first_layer = nn.Linear(dim_vocab, dim_hidden_input)\n",
        "    \n",
        "      # Capa Oculta\n",
        "      self.hidden_layer = nn.Linear(dim_hidden_input, dim_hidden_output)\n",
        "    \n",
        "      # Última Capa\n",
        "      self.last_layer = nn.Linear(dim_hidden_output, num_classes)\n",
        "    \n",
        "      # Función de activación\n",
        "      self.relu = nn.ReLU(inplace=False)\n",
        "      \n",
        "    def forward(self, xs_bow):\n",
        "      \"\"\"Calcula la ultima capa mediante las capas intermedias de la red\n",
        "\n",
        "      Args:\n",
        "        xs_bow: Tensor\n",
        "\n",
        "      Returns:\n",
        "        Tensor con los valores de prediccion\n",
        "      \"\"\"\n",
        "      ## Implementar aquí el forward-pass\n",
        "      \n",
        "      # Hacemos el forward-pass\n",
        "      first_state = self.first_layer(xs_bow)\n",
        "      first_state = self.relu(first_state)\n",
        "\n",
        "      hidden_state = self.hidden_layer(first_state)\n",
        "      hidden_state = self.relu(hidden_state)\n",
        "\n",
        "      last_state = self.last_layer(hidden_state)\n",
        "\n",
        "      return last_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlUntk5IhX26"
      },
      "source": [
        "Ejemplo de prueba para su modelo NN para un batch de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82_O2fAXEclt",
        "outputId": "39357b9e-c801-4b3e-c1f6-3dfbefcd6b03"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[127], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mMyNeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbow_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_hidden_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_hidden_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[1;32m      9\u001b[0m test(batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda())\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    899\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    899\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "test = MyNeuralNetwork(\n",
        "    dim_vocab=len(train_loader.dataset.bow_cols),\n",
        "    num_classes=3,\n",
        "    dim_hidden_input=10,\n",
        "    dim_hidden_output=5).cuda()\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "test(batch[0].cuda())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urgN_j-HlKqm"
      },
      "source": [
        "#### Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NHDkwwlla-H"
      },
      "source": [
        "Consideren las siguientes funciones que les serán utiles. Si lo desea puede modificarlas a su conveniencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj5Ws_0hlYT0"
      },
      "outputs": [],
      "source": [
        "def get_loss(net, iterator, criterion):\n",
        "    net.eval()\n",
        "    total_loss = 0\n",
        "    num_evals = 0\n",
        "    with torch.no_grad():\n",
        "      for xs_bow, labels in iterator:\n",
        "          xs_bow, labels = xs_bow.cuda(), labels.cuda()\n",
        "\n",
        "          logits = net(xs_bow)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "\n",
        "          total_loss += loss.item() * xs_bow.shape[0]\n",
        "          num_evals += xs_bow.shape[0]\n",
        "\n",
        "    return total_loss / num_evals\n",
        "\n",
        "def get_preds_tests_nn(net, iterator):\n",
        "  net.eval()\n",
        "  preds, tests = [], []\n",
        "  with torch.no_grad():\n",
        "    for xs_bow, labels in iterator:\n",
        "      xs_bow, labels = xs_bow.cuda(), labels.cuda()\n",
        "\n",
        "      logits = net(xs_bow)\n",
        "\n",
        "      soft_probs = nn.Sigmoid()(logits)\n",
        "\n",
        "      preds += np.argmax(soft_probs.tolist(), axis=1).tolist()\n",
        "      tests += labels.tolist()\n",
        "\n",
        "    return np.array(preds), np.array(tests)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2ASceXqlblb"
      },
      "source": [
        "A continuación, inicialicen y entrenen su clasificador con los datos de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh2sqTWXGCmK",
        "outputId": "15c723f9-ce5e-4553-a07d-0cc3efff615e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoca 0 completada! Loss: 1.1054631629410911 Accuracy: 0.38235294117647056\n",
            "Epoca 1 completada! Loss: 1.0930101818898146 Accuracy: 0.38235294117647056\n",
            "Epoca 2 completada! Loss: 1.0800085768980139 Accuracy: 0.38235294117647056\n",
            "Epoca 3 completada! Loss: 1.056380967006964 Accuracy: 0.38235294117647056\n",
            "Epoca 4 completada! Loss: 0.9195144391235184 Accuracy: 0.47058823529411764\n",
            "Epoca 5 completada! Loss: 0.7975653784678263 Accuracy: 0.6176470588235294\n",
            "Epoca 6 completada! Loss: 0.6394507672418567 Accuracy: 0.7058823529411765\n",
            "Epoca 7 completada! Loss: 0.4972488123594838 Accuracy: 0.7647058823529411\n",
            "Epoca 8 completada! Loss: 0.5996851416523842 Accuracy: 0.6176470588235294\n",
            "Epoca 9 completada! Loss: 0.33634046876036067 Accuracy: 0.8235294117647058\n",
            "Epoca 10 completada! Loss: 0.2385189199601026 Accuracy: 0.8823529411764706\n",
            "Epoca 11 completada! Loss: 0.0932115251198411 Accuracy: 1.0\n",
            "Epoca 12 completada! Loss: 0.05484988332233008 Accuracy: 1.0\n",
            "Epoca 13 completada! Loss: 0.03735693112727912 Accuracy: 1.0\n",
            "Epoca 14 completada! Loss: 0.02796964330927414 Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "params = {\n",
        "    \"dim_vocab\": len(train_loader.dataset.bow_cols),\n",
        "    \"num_classes\": 3,\n",
        "    \"dim_hidden_input\": 5,\n",
        "    \"dim_hidden_output\": 5,\n",
        "    \"learning_rate\": 0.4,\n",
        "    \"epochs\": 15\n",
        "}\n",
        "\n",
        "# Inicialice su red neuronal\n",
        "net = MyNeuralNetwork(\n",
        "    dim_vocab=params[\"dim_vocab\"],\n",
        "    num_classes=params[\"num_classes\"],\n",
        "    dim_hidden_input=params[\"dim_hidden_input\"],\n",
        "    dim_hidden_output=params[\"dim_hidden_output\"]).cuda()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Definir la Loss = Cross-entropy\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "# Definir el optimizador = SGD: Stochastic-gradient Descent\n",
        "opti = optim.SGD(net.parameters(), lr = params[\"learning_rate\"])\n",
        "\n",
        "# Definir el numero de epocas de entrenamiento\n",
        "epochs = params[\"epochs\"]\n",
        "\n",
        "## Implementar desde aqui el ciclo de entrenamiento\n",
        "## para cada epoca en el conjunto de train\n",
        "for epoch in range(epochs):\n",
        "  for (xs_bow, labels) in train_loader:\n",
        "\n",
        "    opti.zero_grad()\n",
        "\n",
        "    xs_bow, labels = xs_bow.to(device), labels.to(device)\n",
        "\n",
        "    logits = net(xs_bow)\n",
        "\n",
        "    loss = criterion(logits, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    opti.step()\n",
        "\n",
        "  total_loss = get_loss(net, train_loader, criterion)\n",
        "  y_preds, y_tests = get_preds_tests_nn(net, train_loader)\n",
        "  acc = (y_preds == y_tests).sum() / y_preds.shape[0]\n",
        "\n",
        "  print(\"Epoca {} completada! Loss: {} Accuracy: {}\".format(epoch, total_loss, acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAbQjdlrno1o"
      },
      "source": [
        "Pruebe su modelo entrenado con la función `get_preds_tests_nn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1Ncs0lPbYKz",
        "outputId": "79536bb9-82a2-4efe-c628-efe64199fdc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00         7\n",
            "\n",
            "    accuracy                           1.00        34\n",
            "   macro avg       1.00      1.00      1.00        34\n",
            "weighted avg       1.00      1.00      1.00        34\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ya no necesitara calcular gradientes para hacer inferencia\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Calcule el la predicción de su modelo y el ground-truth\n",
        "y_preds, y_tests = get_preds_tests_nn(net, train_loader)\n",
        "print(classification_report(y_tests, y_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-3tp4jR12TR"
      },
      "source": [
        "### Evaluación (0.3 pts.)\n",
        "\n",
        "Ahora probarán el funcionamiento de su clasificador con un conjunto de test.  Habiendo entrenado su clasificador, clasifiquen los documentos del conjunto de prueba `test_set` usando la función `get_preds_tests_nn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71KR1updZ4eL",
        "outputId": "81dc65e2-d3a2-4880-b222-715db6e36019"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89         4\n",
            "           1       1.00      0.83      0.91         6\n",
            "           2       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.92        13\n",
            "   macro avg       0.93      0.94      0.93        13\n",
            "weighted avg       0.94      0.92      0.92        13\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ],
      "source": [
        "y_preds, y_tests = get_preds_tests_nn(net, test_loader)\n",
        "print(classification_report(y_tests, y_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z2BxxUQoINV"
      },
      "source": [
        "Comenten sus resultados. Estudien que ocurre para al menos tres combinaciones de `(dim_hidden_input, dim_hidden_output)`.\n",
        "\n",
        "```\n",
        "Podemos ver que cambiar la cantidad de neuronas en la capa oculta no afecta mucho la precisión del modelo pero si lo rápido que converge. A mayor cantidad de neuronas, el modelo converge más rápido.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Comenten sus resultados. Estudien que ocurre para al menos tres #combinaciones de `(dim_hidden_input, dim_hidden_output)`.\n",
        "\n",
        "def entrenar(dim_hidden_input, dim_hidden_output):\n",
        "\n",
        "    params = {\n",
        "        \"dim_vocab\": len(train_loader.dataset.bow_cols),\n",
        "        \"num_classes\": 3,\n",
        "        \"dim_hidden_input\": dim_hidden_input,\n",
        "        \"dim_hidden_output\": dim_hidden_output,\n",
        "        \"learning_rate\": 0.4,\n",
        "        \"epochs\": 15\n",
        "    }\n",
        "\n",
        "    # Inicialice su red neuronal\n",
        "    net = MyNeuralNetwork(\n",
        "        dim_vocab=params[\"dim_vocab\"],\n",
        "        num_classes=params[\"num_classes\"],\n",
        "        dim_hidden_input=params[\"dim_hidden_input\"],\n",
        "        dim_hidden_output=params[\"dim_hidden_output\"]).cuda()\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Definir la Loss = Cross-entropy\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    # Definir el optimizador = SGD: Stochastic-gradient Descent\n",
        "    opti = optim.SGD(net.parameters(), lr = params[\"learning_rate\"])\n",
        "\n",
        "    # Definir el numero de epocas de entrenamiento\n",
        "    epochs = params[\"epochs\"]\n",
        "\n",
        "    ## Implementar desde aqui el ciclo de entrenamiento\n",
        "    ## para cada epoca en el conjunto de train\n",
        "    for epoch in range(epochs):\n",
        "        for (xs_bow, labels) in train_loader:\n",
        "\n",
        "            opti.zero_grad()\n",
        "\n",
        "            xs_bow, labels = xs_bow.to(device), labels.to(device)\n",
        "\n",
        "            logits = net(xs_bow)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            opti.step()\n",
        "\n",
        "        total_loss = get_loss(net, train_loader, criterion)\n",
        "        y_preds, y_tests = get_preds_tests_nn(net, train_loader)\n",
        "        acc = (y_preds == y_tests).sum() / y_preds.shape[0]\n",
        "\n",
        "        print(\"Epoca {} completada! Loss: {} Accuracy: {}\".format(epoch, total_loss, acc))\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "\n",
        "L = [5, 10, 15, 20, 25]\n",
        "\n",
        "for dim_hidden_input in L:\n",
        "    entrenar(dim_hidden_input, dim_hidden_input)\n",
        "    print(\"\")\n",
        "    y_preds, y_tests = get_preds_tests_nn(net, test_loader)\n",
        "    print(classification_report(y_tests, y_preds))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
